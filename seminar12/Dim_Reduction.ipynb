{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV_1sMP6jEEN"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adasegroup/ML2022_seminars/blob/master/seminar14/seminar_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dkWV8WJjEEQ"
      },
      "source": [
        "# Seminar: Dimensionality reduction\n",
        "\n",
        "#### Seminar structure:\n",
        "\n",
        "* SVD (Singular Value Decomposition) and PCA (Principal Component Analysis) from scratch\n",
        "* Singular Value Decomposition of an Image.\n",
        "* The ` Olivetti Faces dataset` component analysis.\n",
        "* Instrinsic dimensionality estimation.\n",
        "* Manifold learning.\n",
        "* Autoencoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hHydgfwPjEES"
      },
      "outputs": [],
      "source": [
        "#linear algebra\n",
        "import numpy as np\n",
        "#data structures\n",
        "import pandas as pd\n",
        "#ml models\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "#plots\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#beautiful plots\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "#off the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMjFuHcvjEEV"
      },
      "source": [
        "## 1. SVD (Singular Value Decomposition) from the very scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrHdnG9ZjEEW"
      },
      "source": [
        "The singular value decomposition of a matrix $A$ is the factorization of $A$ into the product of three matrices $A = U\\Sigma V^T$ where the columns of $U$ and $V$ are orthonormal and the matrix $\\Sigma$ is diagonal with positive real entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fvvRWiXjEEX"
      },
      "source": [
        "$$\n",
        "X= U\\Sigma V^T=\n",
        "  \\begin{matrix}\n",
        "    \\underbrace{\\left[\\begin{matrix} u_1 &  u_2 &  &  u_r\\end{matrix}\\right.}&\n",
        "    \\underbrace{\\left.\\begin{matrix} u_{r+1} & \\dots &   u_m\\end{matrix}\\right]}\\\\\n",
        "    col(X) & null(X^T)\n",
        "  \\end{matrix}\n",
        "  \\begin{bmatrix}\n",
        "      \\sigma_1 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "         0 & \\sigma_2  & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "         \\dots& & & & &  \\\\\n",
        "         0 & 0 & \\dots & \\sigma_r  & 0 & \\dots & 0 \\\\\n",
        "         0 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
        "         \\dots& & & & &  \\\\\n",
        "         0 & 0 & \\dots & 0 & 0 & \\dots & 0\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "     v_1^T \\\\  v_2^T \\\\ \\dots \\\\  v_r^T \\\\\n",
        "     v_{r+1}^T \\\\ \\dots \\\\  v_n^T\n",
        "  \\end{bmatrix}\n",
        "  \\begin{matrix}\n",
        "    \\left.\\vphantom{\\begin{bmatrix}\n",
        "       \\ v_1^T \\\\  v_2^T \\\\ \\dots \\\\ \\vect v_r^T\n",
        "       \\end{bmatrix}}\\right\\} row(X) \\\\\n",
        "    \\left.\\vphantom{\\begin{bmatrix}\n",
        "      \\ v_{r+1}^T \\\\ \\dots \\\\  v_n^T\n",
        "    \\end{bmatrix}}\\right\\} null(X)\n",
        "  \\end{matrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5VrFrcBjEEY"
      },
      "source": [
        "<img src=\"https://www.askpython.com/wp-content/uploads/2020/11/SVD-1.jpg\" alt=\"Drawing\" style=\"width: 300px;\" />\n",
        "\n",
        "Credit for: https://www.askpython.com/wp-content/uploads/2020/11/SVD-1.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07gbiBkPjEEZ"
      },
      "source": [
        "#### Questions:\n",
        "\n",
        "1. Is `SVD()` use iterative search or there is some closed solution?\n",
        "2. What is the criterion of good decomposition?\n",
        "3. What is the difference of `PCA` and `SVD`?\n",
        "\n",
        "\n",
        "To refresh your math - you can check lectures or here are some useful resources:\n",
        "\n",
        "[1] [Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491#:~:text=What%20is%20the%20difference%20between,PCA%20skips%20less%20significant%20components.)\n",
        "\n",
        "[2] Dan Margalit, Joseph Rabinoff. Interactive Linear Algebra ([link](https://textbooks.math.gatech.edu/ila/index2.html))\n",
        "\n",
        "[3] Strang, Gilbert. Linear Algebra and Its Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7DH4CljEEZ"
      },
      "source": [
        "#### Power Method:\n",
        "\n",
        "In order to implement SVD, we will need *power method*. Recap of power method to compute largest eigenvalue $\\lambda$ and corresponding eigenvector $v$ (for $k$-th step):\n",
        "1. $x_{k+1} \\leftarrow A x_k$\n",
        "2. $x_{k+1} \\leftarrow \\dfrac{x_{k+1}}{ \\| x_{k+1} \\|}$\n",
        "\n",
        "In that case $x_{k+1} \\rightarrow v$ and $A x_{k+1} x^T_{k+1} \\rightarrow \\lambda$ with increasing $k$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How can it help to find SVD?\n",
        "\n",
        "If $A = U \\Sigma V^T$ then $A^TA = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T$ which is a diagonalization of a matrix $A^TA$. Then $\\Sigma^2_{ii} = \\sigma^2_i$ with corresponding eigenvector in first column of $V$."
      ],
      "metadata": {
        "id": "Pulj_yecaILO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UkiTbDELjEEb"
      },
      "outputs": [],
      "source": [
        "class MySVD:\n",
        "    \"\"\"\n",
        "    CLass for iterative SVD search\n",
        "    \"\"\"\n",
        "    def __init__(self, tol=1e-10, eigval_thr=1e-10, max_iter=100, random_state=0, verbose=False):\n",
        "        \"\"\"\n",
        "        Arguements:\n",
        "        - tol: tolerance of difference\n",
        "        - eigval_thr: theshold for eigvalues\n",
        "\n",
        "        \"\"\"\n",
        "        self.tol = tol\n",
        "        self.eigval_thr = eigval_thr\n",
        "        self.max_iter = max_iter\n",
        "        self.gen = np.random.RandomState(random_state)\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _initialize(self, X, n_components):\n",
        "        self.X = np.array(X)\n",
        "        assert self.X.ndim == 2\n",
        "        # singular values\n",
        "        self.eig_vals = []\n",
        "        # left singular vectors\n",
        "        self.eig_vecs_left = []\n",
        "        # right singular vectors\n",
        "        self.eig_vecs_right = []\n",
        "\n",
        "        n_components = self.X.shape[0] if n_components is None else n_components\n",
        "        self.n_components = min(self.X.shape[0], self.X.shape[1], n_components)\n",
        "\n",
        "    def _make_step(self, X, a, b):\n",
        "\n",
        "\n",
        "        ### YOUR CODE HERE ### # make one step of power method\n",
        "\n",
        "\n",
        "        return new_a, new_b\n",
        "\n",
        "    def __call__(self, X, n_components=None):\n",
        "        self._initialize(X, n_components)\n",
        "        # iteretively search for the components\n",
        "        for n_component in range(self.n_components):\n",
        "            a = self.gen.rand(self.X.shape[0])\n",
        "            b = self.gen.rand(self.X.shape[1])\n",
        "\n",
        "            prev_F = np.sum((self.X - a[:, None] * b[None, :])**2)\n",
        "            delta_F = np.inf\n",
        "\n",
        "            # stop criterion\n",
        "            for n_iter in range(self.max_iter):\n",
        "                a, b = self._make_step(self.X, a, b)\n",
        "                F = np.sum((self.X - a[:, None] * b[None, :])**2)\n",
        "                delta = prev_F - F\n",
        "                prev_F = F\n",
        "                self._print('n_eig = {}, n_iter = {}: delta = {:.5f}, F = {:.5f}.'.format(\n",
        "                        len(self.eig_vals), n_iter, delta, F))\n",
        "                if np.abs(delta) <= self.tol:\n",
        "                    self._print('Frobenius norm equals {}. Stopping iterations for n_eig={}.'.format(\n",
        "                        F, len(self.eig_vals)))\n",
        "                    break\n",
        "\n",
        "            eig_val = np.sum(a**2) * np.sum(b**2) # eigen values\n",
        "            # stop criterion\n",
        "            if eig_val < self.eigval_thr:\n",
        "                self._print('Singular value {} equals {}. Stopping iterations.'.format(\n",
        "                    len(self.eig_vals) + 1, eig_val))\n",
        "                break\n",
        "\n",
        "            self.eig_vals.append(eig_val)\n",
        "            self.eig_vecs_left.append(a[:, None] / np.sqrt(np.sum(a**2)))\n",
        "            self.eig_vecs_right.append(b[:, None] / np.sqrt(np.sum(b**2)))\n",
        "            self.X -= a[:, None] * b[None, :]\n",
        "\n",
        "        self.U = np.hstack(self.eig_vecs_left)\n",
        "        self.V = np.hstack(self.eig_vecs_right)\n",
        "        self.D = np.sqrt(self.eig_vals)\n",
        "        return self.U, self.D, self.V.T\n",
        "\n",
        "    def _print(self, msg):\n",
        "        if self.verbose:\n",
        "            print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgzrGG2ojEEd"
      },
      "outputs": [],
      "source": [
        "test_array = np.random.randn(20, 5)\n",
        "u_np, d_np, v_np =  ### YOUR CODE HERE ### # compare with np.linalg\n",
        "u, d, v = MySVD(verbose='True')(test_array)\n",
        "\n",
        "assert np.allclose(d, d_np, atol=1e-2)\n",
        "assert np.allclose(np.dot(np.dot(u, np.diag(d)), v), test_array, atol=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HclD5paQjEEf"
      },
      "outputs": [],
      "source": [
        "class MyPCA:\n",
        "    def __init__(self, n_components=10, **svd_kwargs):\n",
        "        self.n_components=n_components\n",
        "        self.svd = MySVD(**svd_kwargs)\n",
        "\n",
        "    def fit(self, X):\n",
        "        X = np.array(X)\n",
        "        self.n_samples, self.n_features = X.shape\n",
        "\n",
        "        # centering\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        X = X - self.mean_[None, :]\n",
        "\n",
        "\n",
        "        # full variance - as a sum of distances to the center\n",
        "        self.full_variance_ = np.sum(np.mean(X**2, axis=0))\n",
        "\n",
        "        # SVD\n",
        "        self.U, self.D, self.V = ### YOUR CODE HERE ### # compute svd with n_components\n",
        "\n",
        "        # eigen vectors X^TX in V.T\n",
        "        self.n_components_ = len(self.D)\n",
        "        self.components_ = self.V\n",
        "\n",
        "        # the portion of variance explained\n",
        "        self.explained_variance_ = self.D**2 / self.n_samples\n",
        "        self.explained_variance_ratio_ = self.explained_variance_ / self.full_variance_\n",
        "\n",
        "    def transform(self, X):\n",
        "        return  ### YOUR CODE HERE ###\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "    def recover(self, X):\n",
        "        return  ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOQ5tBJfjEEf"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# toy data\n",
        "plt.figure(figsize=(9, 6))\n",
        "np.random.seed(2)\n",
        "X = np.random.randn(200)\n",
        "Y = X + np.random.randn(len(X)) * 0.5\n",
        "model = LinearRegression()\n",
        "model.fit(X.reshape(-1, 1), Y)\n",
        "test_x = np.linspace(-4, 4, 100)\n",
        "predictions = model.predict(test_x.reshape(-1, 1))\n",
        "plt.plot(X, Y, 'o')\n",
        "plt.plot(test_x, predictions, label='No Outliers', color='C0')\n",
        "\n",
        "# add noise\n",
        "sub_inds = np.random.choice(len(X), 10, replace=False)\n",
        "XA = X.copy()\n",
        "YA = Y.copy()\n",
        "YA[sub_inds] = Y[sub_inds] + np.random.rand(len(sub_inds)) * 10\n",
        "model.fit(XA.reshape(-1, 1), YA)\n",
        "predictions = model.predict(test_x.reshape(-1, 1))\n",
        "plt.plot(test_x, predictions, label='With Outliers')\n",
        "plt.plot(XA[sub_inds], YA[sub_inds], 'o', color='C1')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLCrmygqjEEh"
      },
      "outputs": [],
      "source": [
        "X = np.stack([X,Y]).T\n",
        "#X = np.stack([XA,YA]).T\n",
        "\n",
        "pca =  ### YOUR CODE HERE ###\n",
        "pca.fit(X)\n",
        "\n",
        "print(\"Explained variance: {}\".format(pca.explained_variance_ratio_))\n",
        "print(\"Principal components:\")\n",
        "print(\"First: {}\".format(pca.components_[0]))\n",
        "print(\"Second: {}\".format(pca.components_[0]))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], s=25, c='r', marker='o', edgecolor='k')\n",
        "\n",
        "for var_ratio, direction in zip(pca.explained_variance_ratio_, pca.components_):\n",
        "    d = 7 * np.sqrt(var_ratio) * direction\n",
        "    plt.plot([0, d[0]], [0, d[1]], '-k', lw=2)\n",
        "\n",
        "plt.axis('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD0yNbwCjEEi"
      },
      "source": [
        "##### Watch in the next series (seminar): \"Anomaly detection with PCA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G42Po74jEEi"
      },
      "source": [
        "## 2. Singular Value Decomposition and PCA of an Image for data compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6VqRSYnjEEl"
      },
      "outputs": [],
      "source": [
        "# library for images\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open('fruits.png')\n",
        "imggray = img.convert('LA')\n",
        "plt.figure(figsize=(9, 6));\n",
        "plt.imshow(img);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTNJ0XsHjEEl"
      },
      "outputs": [],
      "source": [
        "imgmat = np.array(list(imggray.getdata(band=0)), float)\n",
        "imgmat.shape = (imggray.size[1], imggray.size[0])\n",
        "imgmat = np.array(imgmat)\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.imshow(imgmat, cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J4P9_D6EjEEm"
      },
      "outputs": [],
      "source": [
        "# you can try this one, yet it converges much longer\n",
        "\n",
        "# U, sigma, V = MySVD(verbose=False)(imgmat)\n",
        "U, sigma, V = np.linalg.svd(imgmat, full_matrices=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYrGhm3wjEEm"
      },
      "source": [
        "Computing an approximation of the image using the first column of  $U$  and first row of  $V$  reproduces the most prominent feature of the image, the light area on top and the dark area on the bottom. The darkness of the arch causes the extra darkness in the middle of the reconstruction. Each column of pixels in this image is a different weighting of the same values,  $u_1$ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZDsxBQEjEEn"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "### PCA\n",
        "pca = ### YOUR CODE HERE ###\n",
        "reconstruction = ### YOUR CODE HERE ###\n",
        "\n",
        "### SVD\n",
        "reconstruction = ### YOUR CODE HERE ###\n",
        "plt.imshow(reconstruction, cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfemwUIpjEEn"
      },
      "source": [
        "If we will be able to see the appearance from 2-nd component?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8UZpapKjEEn"
      },
      "outputs": [],
      "source": [
        "# for PCA\n",
        "for i in range(2, 4):\n",
        "    pca =  ### YOUR CODE HERE ###\n",
        "    reconstruction = ### YOUR CODE HERE ###\n",
        "    plt.imshow(reconstruction, cmap='gray')\n",
        "    title = \"n = %s\" % i\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luyrRuPHjEEo"
      },
      "source": [
        "### Ok, how many do we need?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5jn3lijjEEp"
      },
      "outputs": [],
      "source": [
        "# ok how many do we need\n",
        "for i in range(0, 51, 5):\n",
        "    # for PCA\n",
        "    pca = ### YOUR CODE HERE ###\n",
        "    reconstruction =  ### YOUR CODE HERE ###\n",
        "    plt.imshow(reconstruction, cmap='gray')\n",
        "    title = \"n = %s\" % i\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7JLFe8RjEEp"
      },
      "source": [
        "**For my perspective, `[:45]` reconstruction is good enough. Thus we can store less data.**\n",
        "\n",
        "Questions:\n",
        "* What portion of the data we can discard this way?\n",
        "* Can we estimate the intrinsic dimension of the image through PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axMmiVVEjEEp"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA().fit(imgmat)\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.step(### YOUR CODE HERE ###, label='cumulative explained variance')\n",
        "plt.title('Cumulative explained variance', fontsize=16)\n",
        "plt.xlabel('# principal components', fontsize=12)\n",
        "plt.ylabel('Cumulative explained variance', fontsize=12)\n",
        "\n",
        "plt.hlines(y=0.95, xmin=0, xmax= len(pca.components_), colors='r', linestyles='dashed', label='95% explained variance')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OkJIbGqjEEq"
      },
      "source": [
        "## 3. The `Olivetti Faces dataset` component analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElLD6AIQjEEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from numpy.random import RandomState\n",
        "\n",
        "\n",
        "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
        "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezCWr9E5L0AZ"
      },
      "source": [
        "Let's look at the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wXgaVY_LTjA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "for i in range(40):\n",
        "    plt.subplot(4, 10, i + 1)\n",
        "    plt.imshow(data[i].reshape((64, 64)), cmap='gray')\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0cYpWDMM70A"
      },
      "source": [
        "Let's look at the importance of the main components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdu5Zz-M0ix"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA().fit(data)\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.step(range(len(pca.components_)), np.cumsum(pca.explained_variance_ratio_), label='cumulative explained variance')\n",
        "plt.title('Cumulative explained variance', fontsize=16)\n",
        "plt.xlabel('# principle components', fontsize=12)\n",
        "plt.ylabel('Cumulative explained variance', fontsize=12)\n",
        "\n",
        "plt.hlines(y=0.95, xmin=0, xmax= len(pca.components_), colors='r', linestyles='dashed', label='95% explained variance')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwkgnVLRNSqO"
      },
      "source": [
        "Let's look at the principial components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogu1yCghNbpP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(1, 1))\n",
        "plt.imshow(pca.mean_.reshape((64, 64)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(50):\n",
        "    plt.subplot(5, 10, i + 1)\n",
        "    plt.imshow(pca.components_[i].reshape((64, 64)), cmap='gray')\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wgmK0oUONW7"
      },
      "source": [
        "Let's see how does the faces are reconstructed by principial components. For the sake of visibility, we take 10 components, yet is is not enough for good reconstruction:("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxl2XE3aOlh1"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=10)\n",
        "transform_X = pca.fit_transform(data)\n",
        "\n",
        "for i in range(10):\n",
        "    X_hat = pca.inverse_transform(transform_X[i])\n",
        "    plt.figure(figsize=(16, 1))\n",
        "\n",
        "    plt.subplot(1, 13, 1)\n",
        "    plt.imshow(data[i].reshape((64, 64)), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title('face')\n",
        "\n",
        "    plt.subplot(1, 13, 2)\n",
        "    plt.imshow(X_hat.reshape((64, 64)), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title('recon.')\n",
        "\n",
        "    plt.subplot(1, 13, 3)\n",
        "    plt.imshow(pca.mean_.reshape((64, 64)), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title('= 1')\n",
        "\n",
        "    for j in range(10):\n",
        "        plt.subplot(1, 13, 4 + j)\n",
        "        plt.imshow(pca.components_[j].reshape((64, 64)), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title('{}{:.1f}'.format('+' if transform_X[i, j] >= 0 else '',\n",
        "                                    transform_X[i, j]))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPJ7SpZ8PrrF"
      },
      "source": [
        "2D visualization via PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HqaZ59uVjEEx"
      },
      "outputs": [],
      "source": [
        "from matplotlib import offsetbox\n",
        "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "\n",
        "def plot_embedding(X, y, images_small=None, title=None):\n",
        "    \"\"\"\n",
        "    Nice plot on first two components of embedding with Offsets.\n",
        "\n",
        "    \"\"\"\n",
        "    # take only first two columns\n",
        "    X = X[:, :2]\n",
        "    # scaling\n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "    plt.figure(figsize=(13,8))\n",
        "    ax = plt.subplot(111)\n",
        "\n",
        "    for i in range(X.shape[0] - 1):\n",
        "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                 color=plt.cm.RdGy(y[i]),\n",
        "                 fontdict={'weight': 'bold', 'size': 12})\n",
        "        if images_small is not None:\n",
        "            imagebox = OffsetImage(images_small[i], zoom=.4, cmap = 'gray')\n",
        "            ab = AnnotationBbox(imagebox, (X[i, 0], X[i, 1]),\n",
        "                xycoords='data')\n",
        "            ax.add_artist(ab)\n",
        "\n",
        "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
        "        # only print thumbnails with matplotlib > 1.0\n",
        "        shown_images = np.array([[1., 1.]])\n",
        "        for i in range(X.shape[0]):\n",
        "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
        "            if np.min(dist) < 4e-1:\n",
        "                # don't show points that are too close\n",
        "                continue\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIoUpJ4rjEEy"
      },
      "outputs": [],
      "source": [
        "X_projected = ### YOUR CODE HERE ###\n",
        "data_pic = data.reshape((-1, 64, 64))\n",
        "plot_embedding(X_projected, target, data_pic, \"PCA decomposition, projection on first two components  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPi6Di6TjEEy"
      },
      "source": [
        "## 4. Estimation of intrinsic dimension\n",
        "\n",
        "From the paper:\n",
        "\n",
        "\n",
        "*There is a consensus in the high-dimensional data analysis community that the only reason any methods work in very high dimensions is that, in fact, the data are not truly high-dimensional. Rather, they are embedded in a high-dimensional space,\n",
        "but can be efficiently summarized in a space of a much lower dimension, such as a nonlinear manifold.*\n",
        "\n",
        "*Then one can reduce dimension without losing much information for many types of real-life high-dimensional data, such as images, and avoid many of the “curses of dimensionality”. Learning these data manifolds can improve\n",
        "performance in classification and other applications, but if the data structure is\n",
        "complex and nonlinear, dimensionality reduction can be a hard problem.*\n",
        "\n",
        "#####  NIPS 2004: https://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension.pdf\n",
        "\n",
        "### Idea of the method\n",
        "\n",
        "The idea of the nearest-neighbor intrinsic dimension estimation is based on fact of the  that the number of sample points in $\\mathbf{X} = \\mathbb{R}^n$ falling into a ball around $\\mathbf{x}$ is proportional to the radius of of the ball $R$, the dimension of the ball $d$ and the density $f(x)$ for which the well-known estimator exists\n",
        "\n",
        "$$f(x) = \\frac{k/n}{R^d V(d)},$$\n",
        "\n",
        "where $V(d) = \\pi^{d/2} [\\Gamma (d/2+1)]^{-1}$ is the volume of unit sphere in $\\mathbb{R}^d$ and $\\Gamma(d) = (d - 1)!$ is the Gamma function.\n",
        "\n",
        "Then the intrinsic dimension $d$ of the sample can be regressed out or estimated by the maximum likelihood appoarch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ioFUC2nHjEEz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
        "\n",
        "how to use\n",
        "----------\n",
        "\n",
        "The goal is to estimate intrinsic dimensionality of data, the estimation of dimensionality is scale dependent\n",
        "(depending on how much you zoom into the data distribution you can find different dimesionality), so they\n",
        "propose to average it over different scales, the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
        "\n",
        "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
        "\n",
        "Here is one example with swiss roll :\n",
        "\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "X, _ = make_swiss_roll(1000)\n",
        "\n",
        "k1 = 10 # start of interval(included)\n",
        "k2 = 20 # end of interval(included)\n",
        "intdim_k_repeated = repeated(intrinsic_dim_scale_interval,\n",
        "                             X,\n",
        "                             mode='bootstrap',\n",
        "                             nb_iter=500, # nb_iter for bootstrapping\n",
        "                             verbose=1,\n",
        "                             k1=k1, k2=k2)\n",
        "intdim_k_repeated = np.array(intdim_k_repeated)\n",
        "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where\n",
        "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
        "# is (k2 - k1 + 1).\n",
        "\n",
        "\"\"\"\n",
        "# from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def intrinsic_dim_sample_wise(X, k=5):\n",
        "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
        "    dist, ind = neighb.kneighbors(X) # distances between the samples and points\n",
        "    dist = dist[:, 1:] # the distance between the first points to first points (as basis ) equals zero\n",
        "    # the first non trivial point\n",
        "    dist = dist[:, 0:k]# including points k-1\n",
        "    assert dist.shape == (X.shape[0], k) # requirments are there is no equal points\n",
        "    assert np.all(dist > 0)\n",
        "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1]) # dinstance betveen the bayeasan statistics\n",
        "    d = d.sum(axis=1) / (k - 2)\n",
        "    d = 1. / d\n",
        "    intdim_sample = d\n",
        "    return intdim_sample\n",
        "\n",
        "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
        "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
        "    intdim_k = []\n",
        "    for k in range(k1, k2 + 1): # in order to reduse the noise by eliminating of the nearest neibours\n",
        "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
        "        intdim_k.append(m)\n",
        "    return intdim_k\n",
        "\n",
        "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
        "    if random_state is None:\n",
        "        rng = np.random\n",
        "    else:\n",
        "        rng = np.random.RandomState(random_state)\n",
        "    nb_examples = X.shape[0]\n",
        "    results = []\n",
        "\n",
        "    iters = range(nb_iter)\n",
        "    for i in iters:\n",
        "        if mode == 'bootstrap':# and each point we want to resample with repeating points to reduse the errors\n",
        "            #232 111 133\n",
        "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
        "        elif mode == 'shuffle':\n",
        "            ind = np.arange(nb_examples)\n",
        "            rng.shuffle(ind)\n",
        "            Xr = X[ind]\n",
        "        elif mode == 'same':\n",
        "            Xr = X\n",
        "        else:\n",
        "            raise ValueError('unknown mode : {}'.format(mode))\n",
        "        results.append(func(Xr, **func_kw))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188TngW8jEE0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "k1 = 1 # start of interval(included)\n",
        "k2 = 5 # end of interval(included)\n",
        "nb_iter = 3 # more iterations more accuracy\n",
        "# intrinsic_dim_scale_interval gives better estimation\n",
        "intdim_k_repeated = repeated(intrinsic_dim_scale_interval,\n",
        "\n",
        "\n",
        "                             ### YOUR CODE HERE ###, # nb_iter for bootstrapping\n",
        "\n",
        "\n",
        "                             )\n",
        "intdim_k_repeated = np.array(intdim_k_repeated)\n",
        "\n",
        "x = np.arange(k1, k2+1)\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(x, np.mean(intdim_k_repeated, axis=0), 'b', label='Mean') # it is the mean walue\n",
        "plt.fill_between(x,\n",
        "                 np.mean(intdim_k_repeated, axis=0) -\n",
        "                 2 * np.std(intdim_k_repeated, axis=0),\n",
        "                 np.mean(intdim_k_repeated, axis=0) +\n",
        "                 2 * np.std(intdim_k_repeated, axis=0),\n",
        "                 alpha=0.3,\n",
        "                 label='CI=95%',\n",
        "                 color='g')\n",
        "plt.xlabel(\"Nearest Neigbours\")\n",
        "plt.ylabel(\"Intrinsic Dimensionality\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more methods of intrinsic dimensionality estimation we may refer to ```scikit-dimension``` package by this ([link](https://scikit-dimension.readthedocs.io/en/latest/index.html)). It contains different implementations of many methods. Here is the example of estimation using MLE:\n",
        "\n"
      ],
      "metadata": {
        "id": "onT2qb9yhm5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-dimension"
      ],
      "metadata": {
        "id": "bkBrPsFog1fv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skdim\n",
        "int_D = skdim.id.MLE().fit_transform(data)\n",
        "print(int_D)"
      ],
      "metadata": {
        "id": "3Pu2pS4VhE6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc5wqgBzjEE1"
      },
      "source": [
        "## 5. Manifold learning  \n",
        "\n",
        "It is big part of research on the application of geometry and in particular differential geometry to machine learning. If you are interested - next term it will be course by **Prof. Bernstein** - you are welcomed.\n",
        "\n",
        "We are going to touch a couple popular algorithms from: https://scikit-learn.org/stable/modules/manifold.html\n",
        "\n",
        "The manifold learning methods also assumes non-linear algorithms for dimensionality reduction.\n",
        "\n",
        "Questions:\n",
        "* Is `PCA` linear?\n",
        "* Can we compose non-linear `PCA` from the linear?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Independent Component Analysis\n",
        "\n",
        "**Lets go with some other liner method : Independent Component Analysis `ICA`.**\n",
        "FastICA algorithm: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html\n",
        "\n",
        "The problem of blind signal separation.\n",
        "\n",
        "$$X = AS$$\n",
        "\n",
        "where A – mixing matrix, S – source of the signals. The assumptions are:\n",
        "- The source signals are statistically independent of each other.\n",
        "- The values in each source signal have non-Gaussian distributions (at most one)."
      ],
      "metadata": {
        "id": "UqjUQxvh9R98"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otnyAot5jEE2"
      },
      "source": [
        "<img src=\"https://cf2.ppt-online.org/files2/slide/c/CarDgVY6t91bypGImvqBcR3OdAjWhFTZ2JQxL4ofM/slide-6.jpg\" alt=\"Drawing\" style=\"width: 700px;\" />\n",
        "Credit for: https://cf2.ppt-online.org/files2/slide/c/CarDgVY6t91bypGImvqBcR3OdAjWhFTZ2JQxL4ofM/slide-6.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dKRSqSo6jEE3"
      },
      "outputs": [],
      "source": [
        "# let's leave 20 people from faces to get more comprehencible visualisation\n",
        "\n",
        "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
        "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
        "\n",
        "\n",
        "data = data[target <20]\n",
        "target = target[target <20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9reVaTDXjEE3"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "X_projected = FastICA(20, random_state = 42).fit_transform(data)\n",
        "data_pic = data.reshape((-1, 64, 64))\n",
        "plot_embedding(X_projected, target, data_pic, \"ICA decomposition, projection on two components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THxVOEXqjEE3"
      },
      "source": [
        "**The most popular nonlinear algorithms: tSNE,  MDS, Isomap**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEr5uieZjEE3"
      },
      "source": [
        "t-SNE converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
        "\n",
        "1. For each data point $x_i$ we’ll center a Gaussian distribution over that point. Then we measure the density of all points $x_j$ under that Gaussian distribution. Then renormalize for all points. This gives us a set of probabilities $P_{ij}$ for all points in higher dimensional space.\n",
        "\n",
        "2. Get the second set of probablities $Q_{ij}$ for Cauchy distribution (with is Students t-distribution with one degree of freedom), which allow for better modeling of far apart distances, because of heavier tails.\n",
        "\n",
        "3. Map these two sets of probabilities ($ P_{ij}; Q_{ij}$)to each other, optimizing KL-divergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbA1JsJjEE4"
      },
      "source": [
        "Where Kullback–Leibler divergence can be defined as\n",
        "\n",
        "$$ D_{KL} (P || Q) = \\sum P(x) log(\\frac{P(x)}{Q(x)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_5PsxFHjEE5"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, n_iter = 1000, metric='euclidean', learning_rate= 10, verbose=2, random_state = 42)\n",
        "X_projected = tsne.fit_transform(data)\n",
        "data_pic = data.reshape((-1, 64, 64))\n",
        "plot_embedding(X_projected, target, data_pic, \"tSNE decomposition, projection on two components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ch_NV-5ffis"
      },
      "source": [
        "The effect of the perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y0SfXlL-gLtV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3lu_zZrjgO48"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=500, n_features=10, centers=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUgqcfFugQ5i"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "X_hat = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_hat[:, 0], X_hat[:, 1], c=y, cmap='Set1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGGTCgrkgd3Q"
      },
      "source": [
        "Let's see how does the displacement of points in the 2D space changes with respect to the `perplexity` parameter (smoothed effective number of the neighbours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7nE1x1feaY"
      },
      "outputs": [],
      "source": [
        "for perplexity in [1, 5, 15, 30, 50, 75, 100]:\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity)\n",
        "    X_hat = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X_hat[:, 0], X_hat[:, 1], c=y, cmap='Set1')\n",
        "    plt.title('Perp = {}'.format(perplexity))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtUxr2qNiLay"
      },
      "source": [
        "### Multi-dimensional Scaling (MDS)\n",
        "\n",
        "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
        "\n",
        "*Modern Multidimensional Scaling - Theory and Applications, Borg, I.; Groenen P. Springer Series in Statistics (1997)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNrTDJkQiOiP"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n",
        "\n",
        "X_projected = MDS(n_components=2).fit_transform(data)\n",
        "data_pic = data.reshape((-1, 64, 64))\n",
        "plot_embedding(X_projected, target, data_pic, \"MDS decomposition, projection on two components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_WfWr01jEE5"
      },
      "source": [
        "### Isometric Mapping (Isomap)\n",
        "\n",
        "Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Goes in three stages:\n",
        "\n",
        "1. **Nearest neighbor search**.\n",
        "\n",
        "2. **Shortest-path graph search.**\n",
        "\n",
        "3. **MDS with respect to the geodesic distances**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQvbUl4EjEE5"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "X_projected = Isomap(n_components=2).fit_transform(data)\n",
        "data_pic = data.reshape((-1, 64, 64))\n",
        "plot_embedding(X_projected, target, data_pic, \"Isomap decomposition, projection on  two components\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "diploma_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "784c2e2ba9cdfc1ce1e8c565791a35aa78e810f3990b00899de93c9f5ea5b088"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}